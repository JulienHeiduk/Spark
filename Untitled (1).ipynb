{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "os.environ['SPARK_HOME']='C:/Users/Ju/Downloads/spark-2.0.2-bin-hadoop2.7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sys.path.append('C:/Users/Ju/Downloads/spark-2.0.2-bin-hadoop2.7/bin')\n",
    "sys.path.append('C:/Users/Ju/Downloads/spark-2.0.2-bin-hadoop2.7/python')\n",
    "sys.path.append('C:/Users/Ju/Downloads/spark-2.0.2-bin-hadoop2.7/python/pyspark')\n",
    "sys.path.append('C:/Users/Ju/Downloads/spark-2.0.2-bin-hadoop2.7/python/lib')\n",
    "sys.path.append('C:/Users/Ju/Downloads/spark-2.0.2-bin-hadoop2.7/python/lib/pyspark.zip')\n",
    "sys.path.append('C:/Users/Ju/Downloads/spark-2.0.2-bin-hadoop2.7/python/lib/py4j-0.10.3-src.zip')\n",
    "sys.path.append('C:/Java/bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "#import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/Ju/Downloads/spark-2.0.2-bin-hadoop2.7\n"
     ]
    }
   ],
   "source": [
    "print(os.environ['SPARK_HOME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Création du sparkcontext représentant la connexion avec le cluster\n",
    "#Crée automatiquement lorsque qu'on utilise en shell\n",
    "#local => permet à spark de run sur la machine local on one thread sans connexion à un cluster\n",
    "#App name => permet d'idendifier l'application sur l'interface de manage du cluster\n",
    "sc = SparkContext(\"local\", \"App Name2\")\n",
    "\n",
    "#conf = SparkConf().setMaster(\"local\").setAppName(\"My App\")\n",
    "#sc = SparkContext(conf = conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.context.SparkContext object at 0x000000000459E9B0>\n"
     ]
    }
   ],
   "source": [
    "print(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ju\\Downloads\\spark-2.0.2-bin-hadoop2.7\\TEST.md MapPartitionsRDD[1] at textFile at null:-2\n"
     ]
    }
   ],
   "source": [
    "#creation d'un RDD avec les lignes du fichier texte => external dataset\n",
    "#autre methode => distribuant une collection d'objet (list - set ...)\n",
    "lines = sc.textFile(\"C:\\Users\\Ju\\Downloads\\spark-2.0.2-bin-hadoop2.7\\TEST.md\")\n",
    "print(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'12/09/1987,150,rfgergerg'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#first item in this RDD\n",
    "#First line in readme\n",
    "lines.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.RDD'>\n"
     ]
    }
   ],
   "source": [
    "lines_del = lines.map(lambda line: line.split(','))\n",
    "lines_del.take(5)\n",
    "print(type(lines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "df = sqlContext.createDataFrame(lines_del)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: string (nullable = true)\n",
      " |-- _2: string (nullable = true)\n",
      " |-- _3: string (nullable = true)\n",
      "\n",
      "DataFrame[_1: string, _2: string, _3: string]\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_1=u'12/09/1987', _2=u'150'),\n",
       " Row(_1=u'12/09/1987', _2=u'150'),\n",
       " Row(_1=u'13/09/1987', _2=u'1512'),\n",
       " Row(_1=u'13/09/1987', _2=u'11510'),\n",
       " Row(_1=u'14/09/1987', _2=u'11351640')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.registerTempTable(\"table\")\n",
    "df_sql = sqlContext.sql(\"SELECT _1, _2 FROM table\")\n",
    "df_sql.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_1=u'14/09/1987', _3=u'rfgergerg', Count_2=1),\n",
       " Row(_1=u'12/09/1987', _3=u'rfgergerg', Count_2=1),\n",
       " Row(_1=u'13/09/1987', _3=u'rfgergerg', Count_2=2)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sql = sqlContext.sql(\"SELECT _1, _3, COUNT(DISTINCT _2) as Count_2 FROM table GROUP BY _1, _3\")\n",
    "df_sql.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(type(df_sql))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_sql.toPandas().to_csv('mycsv.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           _1         _3  Count_2\n",
      "0  14/09/1987  rfgergerg        1\n",
      "1  12/09/1987  rfgergerg        1\n",
      "2  13/09/1987  rfgergerg        2\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(df_sql.toPandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DataFrame[_1: string, _2: string, _3: string]\n"
     ]
    }
   ],
   "source": [
    "#RDD to Spark DataFrame\n",
    "sparkDF = lines.map(lambda x: str(x)).map(lambda w: w.split(',')).toDF()\n",
    "#Spark DataFrame to Pandas DataFrame\n",
    "pdsDF = sparkDF.toPandas()\n",
    "#df_sql.save('mycsv2.csv', 'com.databricks.spark.csv')\n",
    "#df_sql.saveAsTextFile('mycsv2.csv')\n",
    "#df_sql.write.csv('mycsv3.csv')\n",
    "print(type(sparkDF))\n",
    "print(type(pdsDF))\n",
    "print(sparkDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_1=u'14/09/1987', _3=u'rfgergerg', Count_2=1),\n",
       " Row(_1=u'12/09/1987', _3=u'rfgergerg', Count_2=1),\n",
       " Row(_1=u'13/09/1987', _3=u'rfgergerg', Count_2=2)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparkDF.registerTempTable(\"table2\")\n",
    "df_sql2 = sqlContext.sql(\"SELECT _1, _3, COUNT(DISTINCT _2) as Count_2 FROM table2 GROUP BY _1, _3\")\n",
    "df_sql2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[_1: string, _2: string, _3: string]\n"
     ]
    }
   ],
   "source": [
    "print(sparkDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import StringIO\n",
    "import csv\n",
    "\n",
    "def writeRecords(records):\n",
    "    output = StringIO.StringIO()\n",
    "    writer = csv.DictWriter(output,fieldnames=[\"1\",\"2\"])\n",
    "    for record in records:\n",
    "        writer.writerow(record)\n",
    "    return [output.getvalue()]\n",
    "\n",
    "#df_rdd = df_sql.rdd\n",
    "#finalRdd = doStuff(df_sql)\n",
    "#test = lines.count()\n",
    "#test.saveAsTextFile('test2')\n",
    "#df_sql.mapPartitions(writeRecords).saveAsTextFile('Test.txt',True)\n",
    "#mapPartitions(writeRecords)\n",
    "#df_sql.write.format(\"csv\").save(\"test2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.readwriter.DataFrameWriter at 0x814d748>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " #df_rdd.save('test.csv','parquet')\n",
    "df_sql.write.format(\"com.databricks.spark.csv\")#.save(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#number of items in RDD\n",
    "#opération lancé sur le cluster\n",
    "lines.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'12/09/1987,150,rfgergerg', u'12/09/1987,150,rfgergerg', u'13/09/1987,1512,rfgergerg', u'13/09/1987,11510,rfgergerg', u'14/09/1987,11351640,rfgergerg']\n"
     ]
    }
   ],
   "source": [
    "print(lines.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#filter\n",
    "pythonlines = lines.filter(lambda line:\"1987\" in line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[99] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Pour un RDD à réutiliser => persist() \n",
    "#En pratique utilisé pour conserver un sous ensemble de données en mémoire\n",
    "pythonlines.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pythonlines.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'12/09/1987,150,rfgergerg'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pythonlines.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Exemple de creation RDD sans external data\n",
    "#NB: utilisé que pour learning spark\n",
    "#Necessite d'avoir la donnée sur la machine\n",
    "#RDD1\n",
    "lines = sc.textFile(\"C:\\Users\\Ju\\Downloads\\spark-2.0.2-bin-hadoop2.7\\README.md\")\n",
    "#RDD2\n",
    "lines2 = sc.parallelize({'pandas','i like pandas'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Transformation => implique une cration d'un RDD (exemple filter())\n",
    "#Action => implique un resultat (exmple first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Exemple de transformation et donc création de RDD\n",
    "pythonlines = lines.filter(lambda line:\"Python\" in line)\n",
    "apachelines = lines.filter(lambda line:\"Apache\" in line)\n",
    "pythapachelines = pythonlines.union(apachelines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Input had', 5, 'concerning lines')\n",
      "high-level APIs in Scala, Java, Python, and R, and an optimized engine that\n",
      "## Interactive Python Shell\n",
      "Alternatively, if you prefer Python, you can use the Python shell:\n",
      "# Apache Spark\n",
      "Spark is built using [Apache Maven](http://maven.apache.org/).\n"
     ]
    }
   ],
   "source": [
    "print(\"Input had\", pythapachelines.count() , \"concerning lines\" )\n",
    "for line in pythapachelines.take(5):\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'high-level APIs in Scala, Java, Python, and R, and an optimized engine that',\n",
       " u'## Interactive Python Shell',\n",
       " u'Alternatively, if you prefer Python, you can use the Python shell:',\n",
       " u'# Apache Spark',\n",
       " u'Spark is built using [Apache Maven](http://maven.apache.org/).']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Pour l'exemple. A ne pas utiliser dans un cadre BIG DATA §§§§\n",
    "pythapachelines.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100, 1, 4, 9, 16]\n"
     ]
    }
   ],
   "source": [
    "#Transformation map and filter\n",
    "nums = sc.parallelize([10,1,2,3,4])\n",
    "squared = nums.map(lambda x:x*x).collect()\n",
    "print(squared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'like', 'pandas']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Map: appliqué au 1er element => le retourne sous la forme d'un element\n",
    "words = lines2.map(lambda line:line.split(\" \"))\n",
    "words.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#flatMap: appliqué au 1er element => retourne sous la forme de plusieurs\n",
    "#notion d'iterateur\n",
    "words = lines2.flatMap(lambda line:line.split(\" \"))\n",
    "words.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i like pandas', 'pandas']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#quelques transformations\n",
    "lines2 = sc.parallelize({'pandas','i like pandas','pandas'})\n",
    "lines3 = lines2.distinct()\n",
    "lines3.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i like pandas', 'pandas', 'i like pandas', 'pandas']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines3 = lines2.union(lines2)\n",
    "lines3.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i like pandas', 'pandas']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines3 = lines2.intersection(lines2)\n",
    "lines3.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines3 = lines2.subtract(lines2)\n",
    "lines3.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('i like pandas', 'i like pandas'),\n",
       " ('i like pandas', 'pandas'),\n",
       " ('pandas', 'i like pandas'),\n",
       " ('pandas', 'pandas')]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#toutes les paires possible\n",
    "#très chere en ressoruce\n",
    "lines3 = lines2.cartesian(lines2)\n",
    "lines3.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i like pandas']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sample(withreplacement,fraction,seed)\n",
    "lines3 = lines2.sample(False,0.5,42)\n",
    "lines3.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "#Actions sur RDD\n",
    "sum = nums.reduce(lambda x,y:x+y)\n",
    "print(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {1: 1, 2: 1, 3: 1, 4: 1, 10: 1})"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#collect - count - countByValue\n",
    "nums.countByValue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 1]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums.take(2)\n",
    "#return a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 4]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums.top(2)\n",
    "#top num elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#plus petit elements\n",
    "nums.takeOrdered(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sample en terme de nombre\n",
    "#ici ne retourne pas un rdd cotrainrement à transformation sample\n",
    "nums.takeSample(True,2,42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reduce permet de combiner des elements en parallel\n",
    "nums.reduce(lambda x,y:x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reduce permet de combiner des elements en parallel\n",
    "from operator import add\n",
    "nums.fold(0,add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 5)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqOp = (lambda x, y: (x[0] + y, x[1] + 1))\n",
    "combOp = (lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "nums.aggregate((0, 0), seqOp, combOp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'#', u'# Apache Spark'),\n",
       " (u'', u''),\n",
       " (u'Spark',\n",
       "  u'Spark is a fast and general cluster computing system for Big Data. It provides')]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Chapitre 5 key/value pairs\n",
    "#creation d'un rdd avec systeme clé valeur\n",
    "pairs = lines.map(lambda x:(x.split(\" \")[0],x))\n",
    "pairs.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Transformation sur rdd pairs\n",
    "nums2 = sc.parallelize({(\"1\",2),(\"4\",4),(\"3\",6),(\"3\",61)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1', 2), ('3', 67), ('4', 4)]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums2.reduceByKey(lambda x,y:x+y).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1', <pyspark.resultiterable.ResultIterable at 0x814d828>),\n",
       " ('3', <pyspark.resultiterable.ResultIterable at 0x81785f8>),\n",
       " ('4', <pyspark.resultiterable.ResultIterable at 0x8178f98>)]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums2.groupByKey().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('3', 62), ('3', 7), ('4', 5), ('1', 3)]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums2.mapValues(lambda x:x+1).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 'x'), ('a', 'y'), ('a', 'z'), ('b', 'p'), ('b', 'r')]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = sc.parallelize([(\"a\", [\"x\", \"y\", \"z\"]), (\"b\", [\"p\", \"r\"])])\n",
    "def f(x): return x\n",
    "x.flatMapValues(f).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'b']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.keys().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['x', 'y', 'z'], ['p', 'r']]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.values().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', ['x', 'y', 'z']), ('b', ['p', 'r'])]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.sortByKey().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(u'Spark',\n",
       " u'Spark is a fast and general cluster computing system for Big Data. It provides')"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#subtractByKey - join -rightOuterJoin - leftOuterJoin - cogroup\n",
    "result = pairs.filter(lambda keyvalue:len(keyvalue[1])>20)\n",
    "result.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1', (2, 1)), ('3', (67, 2)), ('4', (4, 1))]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#per key average\n",
    "#clé - nb tot - poids\n",
    "nums2.mapValues(lambda x:(x,1)).reduceByKey(lambda x,y:(x[0]+y[0],x[1]+y[1])).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#combineByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('1', 2), ('3', 61), ('3', 6), ('4', 4)]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums2.sortByKey(ascending = True,numPartitions = None, keyfunc= lambda x:str(x)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {'1': 1, '3': 2, '4': 1})"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Actions\n",
    "nums2.countByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[61, 6]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums2.lookup('3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[180] at mapPartitions at PythonRDD.scala:422"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums2.partitionBy(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Hivecontext\n",
    "#Utilisation des tables de Apache Hive\n",
    "#copier hive-site.xml dans le repertoire spark ./conf/\n",
    "from pyspark.sql import HiveContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#HiveCtx = HiveContext(sc)\n",
    "#rows = hive(\"SELECT name, age FROM users\")\n",
    "#firstRow = rows.first()\n",
    "#print(firstRow.name)\n",
    "\n",
    "#Si json\n",
    "#tweets = hiveCtx.jsonfile(\"tweets.json\")\n",
    "#tweets.registerTempTable(\"tweets\")\n",
    "#results = hiveCtx.sql(\"SELECT name, age FROM tweets\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
